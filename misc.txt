kaggle datasets download -d vikasukani/loan-eligible-dataset -p Source/get_data --unzip

Path(__file__).parent.absolute()


##################################################
TO SEE ITEMS IN A QUEUE:

queue_contents = list(queue.queue)
for content, name in queue_contents:
    data_logger.info('Queue items:\n {}, {}'.format(name, content))

##################################################
ENOUGH TO CREATE TABLES.
COLUMNS WILL BE CREATED BASED ON DATA BEING LOADED:

self.metadata = MetaData()
self.loan_test_table = Table(
 'loan_test',
 self.metadata
)

self.loan_train_table = Table(
 'loan_train',
 self.metadata
)

##################################################
FOR CONCURRENT TASKS:

class Script1:
    def script1_method(self):
        print("Script 1 executed.")

class Script2:
    def script2_method(self):
        print("Script 2 executed.")

def script3_function():
    print("Script 3 executed.")



import concurrent.futures
from script1 import Script1
from script2 import Script2
from script3 import script3_function

def run_scripts_concurrently():
    # Create a ThreadPoolExecutor with max_workers as the number of concurrent tasks you want to run
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        # List of tasks
        tasks = [executor.submit(script1_instance.script1_method),
                 executor.submit(script2_instance.script2_method),
                 executor.submit(script3_function)]

        # Wait for all tasks to complete
        concurrent.futures.wait(tasks)

##################################################
TO START DOCKER FROM YAML WITH .ENV VARIABLES:

docker compose -f docker-compose.yaml --env-file Source/credentials/.env up


##################################################
PRIMARY DOCKERFILE CONFIG:

FROM python:3.10

#RUN pip install -U \
#    pip \
#    setuptools \
#    wheel

ENV POETRY_VERSION=1.8.1
ENV POETRY_HOME=/usr/local
ENV POETRY_VIRTUALENVS_CREATE=false

RUN pip install poetry==$POETRY_VERSION

WORKDIR /app

COPY poetry.lock pyproject.toml ./

RUN poetry install --no-root
RUN pip install --upgrade kaggle

COPY . .

#ENV POETRY_HOME=/opt/poetry
#ENV PATH="${PATH}:${POETRY_HOME}/bin/poetry"

#ENV VIRTUAL_ENV=/opt/venv
#RUN python3 -m venv $VIRTUAL_ENV
#ENV PATH="$VIRTUAL_ENV/bin:$PATH"

ENV KAGGLE_USERNAME=${KAGGLE_USERNAME}
ENV KAGGLE_KEY=${KAGGLE_KEY}

#RUN pip install --no-cache-dir -r requirements.txt
#RUN mkdir -p $HOME/.kaggle
#COPY Source/.kaggle/kaggle.json $HOME/.kaggle

#RUN chmod 600 $HOME/.kaggle/kaggle.json
#RUN ls -la $HOME
#RUN ls -la $HOME/.kaggle/

RUN ls -la /app
RUN ls -la /app/Source
RUN ls -la /app/Source/.kaggle/

RUN chmod +x ./init.sql

RUN echo ~
RUN pwd
RUN whoami

EXPOSE 5432

CMD ["poetry", "run", "python3", "/app/main.py"]

#############################################################
FROM GET_DATA.PY FILE:

# os.environ['KAGGLE_CONFIG_DIR'] = str((KAGGLE_JSON_PATH / 'kaggle.json').absolute())
        # os.chmod(os.environ['KAGGLE_CONFIG_DIR'], 600)
        #
        # if os.environ['KAGGLE_CONFIG_DIR'] and Path(KAGGLE_JSON_PATH / 'kaggle.json').is_file():
        #     with open(KAGGLE_JSON_PATH / 'kaggle.json', 'r', encoding='utf-8') as kaggle_file:
        #         kaggle_credentials = json.load(kaggle_file)
        #     kaggle_credentials.get('username')
        #     kaggle_credentials.get('key')
        #
        #     print('Kaggle username: ', kaggle_credentials.get('username'))
        #     print('Kaggle key: ', kaggle_credentials.get('key'))
        # else:
        #     get_kaggle_credentials().get('KAGGLE_USERNAME')
        #     get_kaggle_credentials().get('KAGGLE_KEY')
        #
        #     print('KAGGLE_USERNAME: ', get_kaggle_credentials().get('KAGGLE_USERNAME'))
        #     print('KAGGLE_KEY: ', get_kaggle_credentials().get('KAGGLE_KEY'))

        # api.authenticate()